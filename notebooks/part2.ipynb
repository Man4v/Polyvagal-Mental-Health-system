{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475ea8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class RobertaAnchorScorer:\n",
    "    def __init__(self, lexicon_csv_path, threshold=0.7):\n",
    "        # Load lexicon (wide format: word,hypo,hyper,flow)\n",
    "        df = pd.read_csv(lexicon_csv_path)\n",
    "        self.lexicon = {}\n",
    "        for _, row in df.iterrows():\n",
    "            word = row[\"word\"].lower().strip()\n",
    "            self.lexicon[word] = {\n",
    "                \"hypo\": float(row.get(\"hypo\", 0.0)),\n",
    "                \"hyper\": float(row.get(\"hyper\", 0.0)),\n",
    "                \"flow\": float(row.get(\"flow\", 0.0))\n",
    "            }\n",
    "\n",
    "        # Anchors = full expanded lexicon\n",
    "        self.anchor_words = list(self.lexicon.keys())\n",
    "\n",
    "        # Load SentenceTransformer (better semantic embeddings than raw RoBERTa)\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Cache for embeddings\n",
    "        self.cache = {}\n",
    "\n",
    "        # Precompute embeddings for all anchor words\n",
    "        self.anchor_embs = self._encode_texts(self.anchor_words)\n",
    "\n",
    "    def _encode_texts(self, texts):\n",
    "        new_texts = [t for t in texts if t not in self.cache]\n",
    "        if new_texts:\n",
    "            embs = self.model.encode(new_texts, convert_to_numpy=True)\n",
    "            for t, e in zip(new_texts, embs):\n",
    "                self.cache[t] = e\n",
    "        return np.array([self.cache[t] for t in texts])\n",
    "\n",
    "    def score_sentence(self, sentence):\n",
    "        STOPWORDS = set(stopwords.words(\"english\"))\n",
    "        tokens = [t for t in word_tokenize(sentence.lower()) if t.isalpha() and t not in STOPWORDS]\n",
    "\n",
    "        if not tokens:\n",
    "            return {\"percentages\": {\"hypo\": 0, \"hyper\": 0, \"flow\": 0},\n",
    "                    \"dominant\": None,\n",
    "                    \"label\": \"No valid tokens\",\n",
    "                    \"matched\": []}\n",
    "\n",
    "        token_embs = self._encode_texts(tokens)\n",
    "\n",
    "        scores = {\"hypo\": 0, \"hyper\": 0, \"flow\": 0}\n",
    "        matched = []\n",
    "\n",
    "        # Compare each token with ALL anchors\n",
    "        for i, tok in enumerate(tokens):\n",
    "            sims = cosine_similarity([token_embs[i]], self.anchor_embs)[0]\n",
    "            for idx, sim in enumerate(sims):\n",
    "                if sim >= self.threshold:\n",
    "                    anchor = self.anchor_words[idx]\n",
    "                    contrib = self.lexicon[anchor]\n",
    "                    for state, val in contrib.items():\n",
    "                        scores[state] += val * sim  # weight by similarity\n",
    "                    matched.append({\n",
    "                        \"token\": tok,\n",
    "                        \"matched_anchor\": anchor,\n",
    "                        \"similarity\": float(sim)\n",
    "                    })\n",
    "\n",
    "        # Normalize into percentages\n",
    "        total = sum(scores.values())\n",
    "        if total == 0:\n",
    "            percentages = {s: 0 for s in scores}\n",
    "            dominant = None\n",
    "            label = \"No relevant keywords found\"\n",
    "        else:\n",
    "            percentages = {s: round(v / total * 100, 2) for s, v in scores.items()}\n",
    "            dominant = max(percentages, key=percentages.get)\n",
    "            label = (f\"Mixed: {percentages['hypo']}% hypo, \"\n",
    "                     f\"{percentages['hyper']}% hyper, \"\n",
    "                     f\"{percentages['flow']}% flow \"\n",
    "                     f\"(dominant: {dominant})\")\n",
    "\n",
    "        return {\n",
    "            \"percentages\": percentages,\n",
    "            \"dominant\": dominant,\n",
    "            \"label\": label,\n",
    "            \"matched\": matched\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f4f396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'percentages': {'hypo': np.float32(0.0), 'hyper': np.float32(0.0), 'flow': np.float32(100.0)}, 'dominant': 'flow', 'label': 'Mixed: 0.0% hypo, 0.0% hyper, 100.0% flow (dominant: flow)', 'matched': [{'token': 'calm', 'matched_anchor': 'calm', 'similarity': 1.0}, {'token': 'relaxed', 'matched_anchor': 'relaxed', 'similarity': 0.9999999403953552}]}\n",
      "{'percentages': {'hypo': np.float32(0.0), 'hyper': np.float32(100.0), 'flow': np.float32(0.0)}, 'dominant': 'hyper', 'label': 'Mixed: 0.0% hypo, 100.0% hyper, 0.0% flow (dominant: hyper)', 'matched': [{'token': 'anxious', 'matched_anchor': 'alarmed', 'similarity': 0.7199091911315918}, {'token': 'anxious', 'matched_anchor': 'anxious', 'similarity': 0.9999999403953552}, {'token': 'anxious', 'matched_anchor': 'fearful', 'similarity': 0.7018227577209473}, {'token': 'scared', 'matched_anchor': 'fearful', 'similarity': 0.7645676136016846}]}\n",
      "{'percentages': {'hypo': np.float32(73.65), 'hyper': np.float32(0.0), 'flow': np.float32(26.35)}, 'dominant': 'hypo', 'label': 'Mixed: 73.6500015258789% hypo, 0.0% hyper, 26.350000381469727% flow (dominant: hypo)', 'matched': [{'token': 'numb', 'matched_anchor': 'numb', 'similarity': 1.0000001192092896}, {'token': 'disconnected', 'matched_anchor': 'connected', 'similarity': 0.759809136390686}, {'token': 'disconnected', 'matched_anchor': 'disconnected', 'similarity': 1.0}]}\n",
      "{'percentages': {'hypo': 0, 'hyper': 0, 'flow': 0}, 'dominant': None, 'label': 'No relevant keywords found', 'matched': []}\n"
     ]
    }
   ],
   "source": [
    "scorer = RobertaAnchorScorer(\"../data/processed/lexicon_for_sentences.csv\")\n",
    "\n",
    "# Example sentences\n",
    "print(scorer.score_sentence(\"I feel calm and relaxed today\"))\n",
    "print(scorer.score_sentence(\"I am very anxious and scared\"))\n",
    "print(scorer.score_sentence(\"I feel numb and disconnected\"))\n",
    "print(scorer.score_sentence(\"Just a random sentence with no keywords\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87c96f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Emotion State Analysis ===\n",
      "Hypoarousal:   0.0%\n",
      "Hyperarousal: 71.13999938964844%\n",
      "Flow:         28.860000610351562%\n",
      "Dominant State: hyper\n",
      "\n",
      "--- Matched Words ---\n",
      "Token:       calm | Anchor:         calm | Similarity: 1.000\n",
      "Token:    anxious | Anchor:      alarmed | Similarity: 0.720\n",
      "Token:    anxious | Anchor:      anxious | Similarity: 1.000\n",
      "Token:    anxious | Anchor:      fearful | Similarity: 0.702\n"
     ]
    }
   ],
   "source": [
    "scorer = RobertaAnchorScorer(\"../data/processed/lexicon_for_sentences.csv\")\n",
    "\n",
    "result = scorer.score_sentence(\"I was mostly calm but got anxious early in the morning.\")\n",
    "\n",
    "# Pretty printing\n",
    "print(\"\\n=== Emotion State Analysis ===\")\n",
    "print(f\"Hypoarousal:   {result['percentages']['hypo']}%\")\n",
    "print(f\"Hyperarousal: {result['percentages']['hyper']}%\")\n",
    "print(f\"Flow:         {result['percentages']['flow']}%\")\n",
    "print(f\"Dominant State: {result['dominant']}\")\n",
    "print(\"\\n--- Matched Words ---\")\n",
    "for m in result[\"matched\"]:\n",
    "    print(f\"Token: {m['token']:>10} | Anchor: {m['matched_anchor']:>12} | Similarity: {m['similarity']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f91bc3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Emotion State Analysis ===\n",
      "Hypoarousal:   0.0%\n",
      "Hyperarousal: 51.220001220703125%\n",
      "Flow:         48.779998779296875%\n",
      "Dominant State: hyper\n",
      "\n",
      "--- Matched Words ---\n",
      "Token: exasperated | Anchor:  exasperated | Similarity: 1.000\n",
      "Token:   grateful | Anchor:     grateful | Similarity: 1.000\n"
     ]
    }
   ],
   "source": [
    "scorer = RobertaAnchorScorer(\"../data/processed/lexicon_for_sentences.csv\")\n",
    "\n",
    "result = scorer.score_sentence(\"These are new words.. exasperated, genuine, grateful\")\n",
    "\n",
    "# Pretty printing\n",
    "print(\"\\n=== Emotion State Analysis ===\")\n",
    "print(f\"Hypoarousal:   {result['percentages']['hypo']}%\")\n",
    "print(f\"Hyperarousal: {result['percentages']['hyper']}%\")\n",
    "print(f\"Flow:         {result['percentages']['flow']}%\")\n",
    "print(f\"Dominant State: {result['dominant']}\")\n",
    "print(\"\\n--- Matched Words ---\")\n",
    "for m in result[\"matched\"]:\n",
    "    print(f\"Token: {m['token']:>10} | Anchor: {m['matched_anchor']:>12} | Similarity: {m['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48e5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
