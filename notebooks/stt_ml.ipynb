{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SpeechRecognition PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press Enter to stop.\n",
      "Recording saved as recording.wav\n",
      "Transcription: ok so let's use some identifiable words so that it can match calm fearful scared annoyed irritated as irritated that enough\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Audio settings\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "WAVE_OUTPUT_FILENAME = \"recording.wav\"\n",
    "\n",
    "def record_audio():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording... Press Enter to stop.\")\n",
    "    frames = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "        # Save audio file\n",
    "        wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "        print(\"Recording saved as\", WAVE_OUTPUT_FILENAME)\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(filename) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(\"Transcription:\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"API request error:\", e)\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input(\"Press Enter to start recording...\")\n",
    "    try:\n",
    "        record_audio()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    text = transcribe_audio(WAVE_OUTPUT_FILENAME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import joblib\n",
    "\n",
    "# Load trained model\n",
    "model = keras.models.load_model(\"../models/speech_emotion_recognition_model.h5\")\n",
    "\n",
    "# Load the label encoder you fit during training\n",
    "label_encoder = joblib.load(\"../models/speech_label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_audio(file_path, max_len=174):\n",
    "    # Load audio (same sr as training)\n",
    "    y, sr = librosa.load(file_path, sr=None, res_type='kaiser_fase')\n",
    "\n",
    "    # Extract MFCCs (same n_mfcc as training)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "\n",
    "    # Transpose to (time, features)\n",
    "    # mfccs = mfccs.T  \n",
    "\n",
    "    # Pad or truncate to fixed length\n",
    "    if mfccs.shape[1] < max_len:\n",
    "        pad_width = max_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "    else:\n",
    "        mfccs = mfccs[:, :max_len]\n",
    "\n",
    "    # Add batch dimension\n",
    "    return np.expand_dims(mfccs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "# Preprocess new audio file\n",
    "X_new = preprocess_audio(\"recording.wav\")\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# Get class index\n",
    "pred_class = np.argmax(y_pred, axis=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: calm\n"
     ]
    }
   ],
   "source": [
    "# Convert index back to label\n",
    "pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "\n",
    "print(\"Predicted class:\", pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class RobertaAnchorScorer:\n",
    "    def __init__(self, lexicon_csv_path, threshold=0.7):\n",
    "        # Load lexicon in format: word,hypo,hyper,flow\n",
    "        df = pd.read_csv(lexicon_csv_path)\n",
    "        self.lexicon = {}\n",
    "        for _, row in df.iterrows():\n",
    "            word = row[\"word\"].lower().strip()\n",
    "            self.lexicon[word] = {\n",
    "                \"hypo\": float(row.get(\"hypo\", 0.0)),\n",
    "                \"hyper\": float(row.get(\"hyper\", 0.0)),\n",
    "                \"flow\": float(row.get(\"flow\", 0.0))\n",
    "            }\n",
    "\n",
    "        # Consider all words in lexicon\n",
    "        self.anchor_words = list(self.lexicon.keys())\n",
    "\n",
    "        # Load SentenceTransformer (better semantic embeddings than raw RoBERTa)\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Cache for embeddings\n",
    "        self.cache = {}\n",
    "\n",
    "        # Precompute embeddings for all anchor words\n",
    "        self.anchor_embs = self._encode_texts(self.anchor_words)\n",
    "\n",
    "    def _encode_texts(self, texts):\n",
    "        new_texts = [t for t in texts if t not in self.cache]\n",
    "        if new_texts:\n",
    "            embs = self.model.encode(new_texts, convert_to_numpy=True)\n",
    "            for t, e in zip(new_texts, embs):\n",
    "                self.cache[t] = e\n",
    "        return np.array([self.cache[t] for t in texts])\n",
    "\n",
    "    def score_sentence(self, sentence):\n",
    "        STOPWORDS = set(stopwords.words(\"english\"))\n",
    "        tokens = [t for t in word_tokenize(sentence.lower()) if t.isalpha() and t not in STOPWORDS]\n",
    "\n",
    "        if not tokens:\n",
    "            return {\"percentages\": {\"hypo\": 0, \"hyper\": 0, \"flow\": 0},\n",
    "                    \"dominant\": None,\n",
    "                    \"label\": \"No valid tokens\",\n",
    "                    \"matched\": []}\n",
    "\n",
    "        token_embs = self._encode_texts(tokens)\n",
    "\n",
    "        scores = {\"hypo\": 0, \"hyper\": 0, \"flow\": 0}\n",
    "        matched = []\n",
    "\n",
    "        # Compare each token with ALL anchors\n",
    "        for i, tok in enumerate(tokens):\n",
    "            sims = cosine_similarity([token_embs[i]], self.anchor_embs)[0]\n",
    "            for idx, sim in enumerate(sims):\n",
    "                if sim >= self.threshold:\n",
    "                    anchor = self.anchor_words[idx]\n",
    "                    contrib = self.lexicon[anchor]\n",
    "                    for state, val in contrib.items():\n",
    "                        scores[state] += val * sim  # weight by similarity\n",
    "                    matched.append({\n",
    "                        \"token\": tok,\n",
    "                        \"matched_anchor\": anchor,\n",
    "                        \"similarity\": float(sim)\n",
    "                    })\n",
    "\n",
    "        # Normalize into percentages\n",
    "        total = sum(scores.values())\n",
    "        if total == 0:\n",
    "            percentages = {s: 0 for s in scores}\n",
    "            dominant = None\n",
    "            label = \"No relevant keywords found\"\n",
    "        else:\n",
    "            percentages = {s: round(v / total * 100, 2) for s, v in scores.items()}\n",
    "            dominant = max(percentages, key=percentages.get)\n",
    "            label = (f\"Mixed: {percentages['hypo']}% hypo, \"\n",
    "                     f\"{percentages['hyper']}% hyper, \"\n",
    "                     f\"{percentages['flow']}% flow \"\n",
    "                     f\"(dominant: {dominant})\")\n",
    "\n",
    "        return {\n",
    "            \"percentages\": percentages,\n",
    "            \"dominant\": dominant,\n",
    "            \"label\": label,\n",
    "            \"matched\": matched\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Emotion State Analysis ===\n",
      "Hypoarousal:   0.0%\n",
      "Hyperarousal: 88.54%\n",
      "Flow:         11.46%\n",
      "Dominant State: hyper\n",
      "\n",
      "--- Matched Words ---\n",
      "Token:       calm | Anchor:         calm | Similarity: 1.000\n",
      "Token:    fearful | Anchor:      anxious | Similarity: 0.702\n",
      "Token:    fearful | Anchor:      fearful | Similarity: 1.000\n",
      "Token:     scared | Anchor:      fearful | Similarity: 0.765\n",
      "Token:    annoyed | Anchor:      annoyed | Similarity: 1.000\n",
      "Token:    annoyed | Anchor:    irritated | Similarity: 0.779\n",
      "Token:  irritated | Anchor:      annoyed | Similarity: 0.779\n",
      "Token:  irritated | Anchor:    irritated | Similarity: 1.000\n",
      "Token:  irritated | Anchor:      annoyed | Similarity: 0.779\n",
      "Token:  irritated | Anchor:    irritated | Similarity: 1.000\n"
     ]
    }
   ],
   "source": [
    "scorer = RobertaAnchorScorer(\"../data/processed/lexicon_for_sentences.csv\")\n",
    "\n",
    "result = scorer.score_sentence(text)\n",
    "\n",
    "# Pretty printing\n",
    "print(\"\\n=== Emotion State Analysis ===\")\n",
    "print(f\"Hypoarousal:   {result['percentages']['hypo']}%\")\n",
    "print(f\"Hyperarousal: {result['percentages']['hyper']}%\")\n",
    "print(f\"Flow:         {result['percentages']['flow']}%\")\n",
    "print(f\"Dominant State: {result['dominant']}\")\n",
    "print(\"\\n--- Matched Words ---\")\n",
    "for m in result[\"matched\"]:\n",
    "    print(f\"Token: {m['token']:>10} | Anchor: {m['matched_anchor']:>12} | Similarity: {m['similarity']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
